Things to evaluate:
1. Whether or not to put word2vec as retrieval level match feature
2. tf-ifd caculation: stop words removal and low freq words removal
3. Which pre-trained word2vec to use.
4. Number of latent states in LDA
5. Performance of LDA is very stochastic. Need to re-run several times to get a good model.
Need to develop a method to evaluate.
6. Evaluate LSI vs. LDA
7. Try rank SVM using different kernels
8. Fusion using boosted stumps
9. Confidence score by platt scaling
10. Modify topic word model
11. translation model
12. generate rank training data based on wrong top answers running on training and test.!!


CV experiments:

python ir_query_engine/evaluation/eval_models.py -d medium_training.json --eval_tfidf --num_folds=4
(1.0, 1.0000000104974742, 0.4947035005571766, 0.72007993946099025)
python ir_query_engine/evaluation/eval_models.py -d medium_training.json --eval_tfidf --num_folds=10
(1.0, 1.0000000112862431, 0.5490693673695893, 0.73909327604276664)

[(0, 7917), (7917, 15834), (15834, 23752)]

source venv/bin/activate
export PYTHONPATH=$(pwd)

python ir_query_engine/main.py -d medium_training.json --generate_rank_training_data --pair=0,2124 --write_rank_data
python ir_query_engine/main.py -d medium_training.json --generate_rank_training_data --pair=2124,4249 --write_rank_data