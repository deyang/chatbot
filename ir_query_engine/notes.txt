Things to evaluate:
1. Whether or not to put word2vec as retrieval level match feature
2. tf-ifd caculation: stop words removal and low freq words removal
3. Which pre-trained word2vec to use.
4. Number of latent states in LDA


5. Performance of LDA is very stochastic. Need to re-run several times to get a good model.
Need to develop a method to evaluate.


[(0, 6021), (6021, 12042), (12042, 18063), (18063, 24084), (24084, 30105), (30105, 36128)]

source venv/bin/activate
export PYTHONPATH=$(pwd)
python ir_query_engine/main.py -d large_training.json --train_rank_model --seq --pair=0,6021

source venv/bin/activate
export PYTHONPATH=$(pwd)
python ir_query_engine/main.py -d large_training.json --train_rank_model --seq --pair=6021,12042

source venv/bin/activate
export PYTHONPATH=$(pwd)
python ir_query_engine/main.py -d large_training.json --train_rank_model --seq --pair=12042,18063

source venv/bin/activate
export PYTHONPATH=$(pwd)
python ir_query_engine/main.py -d large_training.json --train_rank_model --seq --pair=18063,24084

python ir_query_engine/main.py -d large_training.json --train_rank_model --seq --pair=24084,30105

python ir_query_engine/main.py -d large_training.json --train_rank_model --seq --pair=30105,36128